# Authorship Attribution for Neural Text Generation

[Run App](https://apporfinder-grtlgb4yxauid3dye9l9wd.streamlit.app/)

## Introduction 

With advancements in chatbot technologies that mimic human language, there's a growing potential for these models to be used in deep fakes. In this project, we analyze various prior works to answer three key questions:
- **Same or Not**: Whether the generated text is from the same NLG method (human) or not.
- **Human vs Bot**: Whether the text is from a human or an NLG method.
- **Which NLG Method**: Whether the text is from a specific NLG method from a set of methods.

These questions are typical binary or multi-label classification problems. The papers reviewed deal with the domain of text classification using various Machine Learning and Deep Learning models such as RNNs.

## Dataset

In our study, we investigated three primary tasks using datasets from ten Natural Language Generation (NLG) methods: CTRL, GPT, GPT2, GPT3, Instruct GPT, GROVER, XLM, XLNET, PPLM, FAIR, and human-generated content. Our objectives were:
- **Determining Correspondence**: Assessing if two pieces of text are correlated.
- **Classifying Text Types**: Differentiating between two distinct classes of text.
- **Identifying NLG Methods**: Recognizing the specific NLG method behind a given text.

We prepared both balanced (1:1 ratio) and imbalanced (1:10 ratio) datasets to evaluate our models. Additionally, we conducted a comprehensive Reddit case study involving GPT3, InstructGPT, and human contributions across various topics using balanced (1:1 ratio) and imbalanced (1:2 ratio) datasets.

## Methods Used

We employed a range of methodologies in our research, including:

### First Method
Focused on preprocessing steps such as removing punctuations, eliminating stopwords, and lowercase conversion. The model used was a two-layer BiLSTM network.

### Second Method
Differed from the first by using BERT tokenizer and the same BiLSTM architecture.

### Third Method
Similar to the second method, using BERT tokenizer and extracting stylometric features, integrated into the BiLSTM model.

### Fourth Method
Combined stylometric analysis with BERT tokenizer and a modified BiLSTM architecture, including global max-pooling, dropout, and batch normalization layers.

### Fifth Method
Implemented a grid search with XGBoost classifier model and used SMOTE for data imbalance.

## Flow Chart

![Flow Chart](https://github.com/HarinathCingapuram94/AuthorFinder/assets/60059816/67f285de-f449-44b5-8908-1a0ad88d246b)

## Results

![Result 1](https://github.com/HarinathCingapuram94/AuthorFinder/assets/60059816/7f927a57-0df3-4d84-bebc-29973744c2af)

![Result 2](https://github.com/HarinathCingapuram94/AuthorFinder/assets/60059816/e43e2eee-3f3c-4fdf-87f0-deedec26504b)

![Result 3](https://github.com/HarinathCingapuram94/AuthorFinder/assets/60059816/01a5eeaf-3efd-42c2-a748-edef4923099e)

## Conclusion

Our study's implemented models exhibited noteworthy performance across a variety of tasks, displaying particular strengths in certain areas. Notably, in some cases, models developed by our team and the Random Forest model surpassed others in effectiveness, highlighting their robustness and efficiency in handling complex classification tasks.

A key discovery from our research was the effectiveness of stylometric features in extracting meaningful information from text data. These features, when combined with Part-of-Speech (POS) estimation, provided deep insights into the grammatical and syntactic structures of the chat data. This combination proved particularly valuable in discerning subtle nuances that differentiate human-generated texts from those produced by NLG methods.

Furthermore, the incorporation of a robust pre-trained tokenizer, such as BERT, significantly enhanced our models' performance. This underscores the potential benefits of integrating traditional stylometric analysis with advanced deep learning techniques in authorship attribution tasks. The synergy between these approaches contributed significantly to the accuracy and reliability of our models.

These findings are not just a testament to the success of our current research but also pave the way for future exploration in this field. They suggest potential avenues for further improvements and novel approaches that can be pursued in future studies. Overall, our research contributes valuable insights to the domain of authorship attribution, underscoring the importance of combining diverse methodologies to tackle the challenges presented by NLG technologies.

## Features

- **Model Selection**: Choose from different models including XGBoost and BiLSTM for classification.
- **NLG Method Detection**: Capable of distinguishing texts generated by various NLG methods like GPT-2, GPT-3, etc.
- **User-Friendly Interface**: Simple and intuitive UI for easy interaction with the application.

## How It Works

The application uses a combination of TensorFlow models and NLP techniques to analyze the input text. It preprocesses the data using NLTK, vectorizes it, and then feeds it into the selected machine learning model for classification.

## Technologies Used

- Streamlit for web app development
- TensorFlow and Keras for model building and predictions
- NLTK for natural language processing
- Scikit-learn for additional machine learning utilities

## Setup and Installation

For an in-depth understanding of the methodologies and technologies used in this project, refer to our [Research Paper](https://drive.google.com/file/d/1STTkT4chq314ALw1R3VVPjgmVmxFWBjZ/view?usp=sharing).
A detailed overview of the project can be found in our [Presentation](https://drive.google.com/file/d/1r7W4T2jD2-hCr2d0ZY09Wkoh70M-6BKl/view?usp=sharing).
